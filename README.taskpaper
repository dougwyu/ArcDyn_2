- This file records notes about what i have done with the ArcDyn files, and also lists to dos
ArcDyn paper recode 2019:
	- redownload the files from Amazon S3
		- create repo
		- start restore process
		- re-org the sequence data to prep for upload to datadryad:  create a single folder holding all sequence files.  @done(2019-01-12)
		- download from S3
		- run fastq_combine scripts
		- run fastqc
		- upload fastq files to SRA
	- clean up and revise the bash and R code to run on the re-organised sequence files
		- # CHECK THIS:  
			The sequencing of plates G and H for quotation HEL.TR.ENQ-2379.B.01 has now been completed and a summary of the raw data is attached.  Please note that your sample references have been taken from the attached sample information form (SIF), assuming that samples S1 – S96 are Plate G and S97 onwards (well “A13”) are plate H.   There are 2 samples in Plate H which didn’t generate reads, from wells C2 and H12; according to the SIF, Plate H well H12 (sample S192) contains a blank. 
		- find the origin of the single 1998 sample that is out of sequence in Table S4.2.  
		- especially fix the bit of code checking the ratios of the spikes in 4_idxstats_tabulate_macOS_PlatesGH.Rmd  (should indeed be 1:2:4)
		- merge and rewrite code in 4 _idxstats_tabulate_macOS_PlatesGH.Rmd and S0_clean_data_files.r to produce input_data_step5.RData.  There are 10 files.  These 10 files should be the output of 4_idxstats_tabulate_macOS_PlatesGH.Rmd or of 5_merge_idx_meta_genomecov.Rmd (not sure which one.  i added Otso's code to 4_idxstats_tabulate_macOS_PlatesGH.Rmd, but i think it will make more sense to add to 5_merge_idx_meta_genomecov.Rmd.  
		- create a new repo with the new code, which can be published on github
		- create column to allow removal of these taxa: c("Sarcoptiformes", "Trombidiformes", "Entomobryomorpha", "Neelipleona", "Poduromorpha", "Symphypleona")   
	- management tasks
		- after i successfully re-run with the PlatesAB, i can delete the redundant folder amazon-working-douglasyu/Earlham_soups_20160910/ 
		- update content of:  amazon-working-douglasyu/readme_Amazon_glacier_file_descriptions_20180128
Archived To Dos that i should add to above:
	- go over code in fine detail and see if i can find any bugs:  especially in the table joining steps
	- list which samples produced very little or no data, check against the library QC information, and see if any are legitimately missing or i can ask EI to resupply
		- Darren has sent me library QC information for GH.  look up A2B2, EF:  
			- LITE update G & H 20171023.rtfd
					- Plate G 
						A12
						B03
						F07
						F10
						D10
						B10
						E10
						B08
					- Plate H  
						C08
						B08
						F01
						G03
						G09
						C07
						G06
						C05
						B07
						D05
						H12
						G01
						C02
						B04
	- bwa against mitogenomes and barcodes and compare
		- http://homer.ucsd.edu/homer/basicTutorial/mapping.html
		- bwa index -a bwtsw genome.fa # build index.  What is bwtsw?
		- bwa mem -t $NPROC genome.fa reads1.fq reads2.fq | samtools view -F 0x4 -b | samtools sort > aln-pe.bam # 
		- samtools depth -a instead of bedtools genomecov -d
	- Pardosa glacialis missing from mock samples.  WHY?  @flag
		- extract Pardosa-mapped reads from a bam file (htsbox bam2fq?)  that has lots of Pardosa in it and use those to map against the mock samples
	- ArcDyn:  genes and primers for the non-mitogenome species @flag @done(2019-01-12)
	- make textfile for taxonomy (class, order, family, genus, species) of each mitogenome and join to idx_meta_genomecov @done(2019-01-12)
Records:
	- A2B2 missing files
		- situation as of 20180219
			- I think that 16 samples are still running on lane 11
			- 8 samples produced no data but were included in the already-run 10 lanes and thus produced no fastq files
			- 168 samples successfully produced at least some data
			- 16+9+168 = 192 samples
			- of the 168 samples, 15 of the samples produced such little data that they are being omitted at the R stage
			- of the 24 samples that produced zero (n=8) or little (n=16) data, it might be the case the PlatesAB produced data for those samples. This appears to be the list of omitted samples
				[1] "1997AUG20_Art3_TrapA_Wk34" "1997JUL08_Art3_TrapA_Wk28" "2011JUL14_Art3_TrapC_Wk28" "2012AUG19_Art3_TrapA_Wk33"
				[5] "2013AUG26_Art3_TrapA_Wk35" "2013JUL09_Art3_TrapB_Wk28" "1997AUG05_Art3_TrapC_Wk32" "2013AUG26_Art3_TrapB_Wk35"
				[9] "1998JUL29_Art3_TrapC_Wk31" "1999AUG26_Art3_TrapA_Wk34" "2011JUL08_Art3_TrapB_Wk27" "2011JUL22_Art3_TrapC_Wk29"
				[13] "2011AUG25_Art3_TrapA_Wk34" "1999JUL29_Art3_TrapC_Wk30" "2011JUL01_Art3_TrapA_Wk26" "2013JUN17_Art3_TrapB_Wk25"
				[17] "2012SEP02_Art3_TrapB_Wk35" "2011AUG16_Art3_TrapA_Wk33" "2011AUG06_Art3_TrapC_Wk31" "2011AUG06_Art3_TrapB_Wk31"
				[21] "2013JUL22_Art3_TrapC_Wk30" "2012AUG19_Art3_TrapB_Wk33" "2013JUL29_Art3_TrapC_Wk31" "1997JUL08_Art3_TrapC_Wk28"
			- information analysed in:  2017/bulk_samples/platesA2B2/ENQ-1643_PIP-1744_PSEQ-1600_and_PSEQ-1618_datasum_23.01.18.xlsx
	- check number of samples in A2B2;  should be 168.  
		- outcome:  yes.  i downloaded 168 sets of fastq files.  
	- convert some bam files to paf format to look at distribution of mapping qualities (https://github.com/lh3/htsbox):  htsbox samview -p in.bam > out.paf.
		- https://github.com/lh3/miniasm/blob/master/PAF.md # for PAF format
		- outcome:  60 is indeed the max value, and looking at the histogram of mapping values, i have decided to re-run with -q 48
	- view the bam files:  mapping to NNNNN's??  
		- outcome:  this is known as hard masking, and BWA does not map to these parts.  minimap2 also appears to avoid them in the counting of the alignment 
	- samtools view -F 0x4 removes UNMAP reads, which reduces the file sizes to 1% of main bam. Thus, samtools filter -F 0x4 the minimap2 outputs
		- outcome:  re-running minimap2 to reduce size of files. filtered through minimap2_COIBarcodes only sam files to remove unmapped. These unmapped reads are just a copy of the reads in the fq files
	- run R code on AB, A2B2, GH, and EF and compare with Yinqiu's results and also the COI spike and the positive controls
		- write R code to process the samtools outputs:  idxstats_tabulate
			-  Read in all idx_stats files, add sample metadata to columns, and merge into one table in long format (vertically by time)
			-  merge with sample excel workbook to add the sample date information
			-  go through the genomecov files and calculate the mean and standard deviation of coverage for each mitogenome/sample and add the columns to the idx table
			-  make wide table using tidyr (the column data format should be DateTrap (e.g. 1998_08_05_TrapA) in temporal order).
			-  confirm that i have the correct number of samples (compare with number of input files) e.g. PlatesAB only have 176 bam files (answer:  i only initially downloaded 176 bam files for code testing)
			-  Debug the bug that causes sample to have repeated lines, using a tabulate command to check that I don't have repeated lines.  Answer: re-run the minimap2/samtools/bedtools scripts and redownload the idxstats files.  iit wasn't a bug.  I had put the same idxstats files into multiple BWA folders in my test folders
			-  create lysis buffer datasets, download idxstats and genomecov data, run R code on Plates AB, A2B2, EF, GH
		-  platesGH:  combine_fastq, fastQC, multiqc (using PKG-ENQ-2379-Data_Transfer-PSEQ-1586-trimmed), remove orig fastq.gz files.  
		- remove original, noncombined fastq.gz files from A2B2 and GH when i'm done mapping and checking.
		-  platesA2B2: combine_fastq, fastQC, multiqc, remove orig fastq.gz files
		-  minimap2/samtools for platesGH
		-  minimap2/samtools for platesA2B2
		- download samtools outputs for minimap2 to mitogenomes for q60 (F2308_f0x2_q60)
		- finish running minimap2 and samtools scripts:  download PlatesEF samtools outputs for minimap2 to barcode COIs
		- save folders with the bamfiles and samtools outputs for minimap2 to barcode COIs
		- Compare PlatesAB and A2B2 WITH YINQIU'S BWA_results_20161025.xlsx.  
		- Compare PlatesGH, PlatesEF with the positive controls, the seasonal patterns, the COI spikes
		- analyse minimap2 against mitogenomes for F2308 f0x2 q60
		- analyse minimap2 against barcodes_only for F2308, q1 and q60
		- re-run minimap2 against mitogenomes to re-generate the bam files?  benefit is that i can then run further samtools at any time. cost is storage space
		- outcome:  i have successfully produced output files for Otso.  now the job is to refine and solve the pardosa problem
		- visualise minimap2 bam files and check that i am not mapping to the Ns between the protein-coding genes. SHOULD I SUBTRACT A FIXED NUMBER FROM EACH MITOGENOME LENGTH TO CALCULATE THE CORRECT LENGTH OF THE CODING PORTION?  
	- re run minimap2/samtools/bedtools/R pipeline with F2308 q48 and compare with F2308 q60
		- q48 produces maybe 5-10% more reads and correspondingly more reads for false positives (although the Coef vars for false positives are still high). There is not much benefit nor much cost to using q48.  note that I did not filter for PROPER PAIRS, which should reduce the false positives
	- run PlatesGH against 19 positive control species and look at the mocks (20180223)
		- all 19 species detected except for Pardosa glacialis
		- within species ratios are accurate
		- across species gradient is roughly accurate
		- CVs are low (below 1.5)
		- upshot:  we can detect even low biomass fraction species (although in this run, i did not map against other mitogenomes so did not test directly for false positives)
	- 20180223: re-run PlatesA2B2, adding the final Lane 11
		- added the final fastq files (after the trimming step)
		- re-mapped and samtools filtered.  these output files are now the new output files
		- Lanes 1-11 all have all the samples, so adding Lane 11 did not increase number of samples but did increase by around 10% the amount of sequence data compared to the Lanes 1-10 only run
	- 20180227: run pipeline against new mitogenome reference database containing 307 species
	- 20180228: send 307 mitogenomes to Tea, annotated, so they include any 16S and 12S genes assembled
	- 20180301: download B2 files and upload to S3 Glacier for long-term storage